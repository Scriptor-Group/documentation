# Chat Completions API

Documentation de l'endpoint `/v1/chat/completions` pour g√©n√©rer des r√©ponses avec vos agents IA.

**Endpoint :** `POST https://api.devana.ai/v1/chat/completions`
**Compatible :** OpenAI Chat Completions API + extensions Devana.ai

---

## üìë Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Authentification](#authentification)
3. [Param√®tres de la requ√™te](#param√®tres-de-la-requ√™te)
4. [Exemples d'utilisation](#exemples-dutilisation)
5. [Streaming](#streaming)
6. [RAG & Sources](#rag--sources)
7. [Tools Calls](#tools-calls)
8. [Gestion des erreurs](#gestion-des-erreurs)
9. [Param√®tres avanc√©s](#param√®tres-avanc√©s)

---

## Vue d'ensemble

L'endpoint `/v1/chat/completions` est compatible avec l'API OpenAI tout en offrant des fonctionnalit√©s avanc√©es :

**Fonctionnalit√©s principales :**

- ‚úÖ Gestion des historiques de conversations
- ‚úÖ Support des fichiers en pi√®ces jointes
- ‚úÖ RAG (Retrieval Augmented Generation) automatique
- ‚úÖ Tools calls avec validation et contr√¥le
- ‚úÖ Streaming de r√©ponses (Server-Sent Events)
- ‚úÖ Configuration avanc√©e par agent
- ‚úÖ Metadata et webhooks pour analytics
- ‚úÖ Mode debug pour d√©veloppement
- ‚úÖ Retry automatique avec backoff exponentiel

---

## Authentification

Incluez votre cl√© API dans le header `Authorization` :

```bash
Authorization: Bearer VOTRE_TOKEN
```

Le syst√®me :

- Hydrate automatiquement l'utilisateur √† partir du token
- V√©rifie les droits d'acc√®s aux agents

## Effectuer des requ√™tes

### Endpoint

```
POST /v1/chat/completions
```

### Corps de la Requ√™te

| Param√®tre              | Type         | Obligatoire | Description                                                                                                  |
| ---------------------- | ------------ | ----------- | ------------------------------------------------------------------------------------------------------------ |
| `model`                | string       | Oui         | Un ID d'agent cr√©√© dans le syst√®me.                                                                          |
| `messages`             | array        | Oui         | Tableau des messages composant la conversation (maximum 4 messages).                                         |
| `temperature`          | float        | Non         | Contr√¥le l'al√©atoire dans les r√©ponses. Valeurs entre 0 et 1. D√©faut: 0.8 ou valeur configur√©e pour l'agent. |
| `max_tokens`           | integer      | Non         | Nombre maximum de tokens √† g√©n√©rer.                                                                          |
| `top_p`                | float        | Non         | Contr√¥le la diversit√© via l'√©chantillonnage nucleus.                                                         |
| `n`                    | integer      | Non         | Nombre de completions √† g√©n√©rer.                                                                             |
| `stop`                 | string/array | Non         | S√©quences o√π l'API arr√™tera la g√©n√©ration.                                                                   |
| `stopSequences`        | array        | Non         | S√©quences additionnelles o√π l'API arr√™tera la g√©n√©ration.                                                    |
| `frequency_penalty`    | float        | Non         | P√©nalit√© de fr√©quence pour r√©duire la r√©p√©tition.                                                            |
| `presence_penalty`     | float        | Non         | P√©nalit√© de pr√©sence pour encourager la nouveaut√©.                                                           |
| `stream`               | boolean      | Non         | Si vrai, les deltas de messages partiels seront envoy√©s.                                                     |
| `conversation_id`      | string       | Non         | ID de la conversation existante √† continuer.                                                                 |
| `response_format`      | string       | Non         | Format de r√©ponse souhait√© (par exemple, { type: "text" \| "json_object"}). (compatible selon le LLM actif)  |
| `lang`                 | string       | Non         | Langue pour la r√©ponse. Par d√©faut, locale de l'utilisateur ou 'fr'.                                         |
| `files`                | array        | Non         | Tableau des IDs de fichiers √† inclure dans le contexte.                                                      |
| `clientModel`          | string       | Non         | Mod√®le sp√©cifique √† utiliser avec la configuration Devana AI. D√©faut: valeur de DEFAULT_MODEL.               |
| `custom`               | object       | Non         | Options de configuration personnalis√©es, comme `disableAutomaticIdentity`.                                   |
| `metadata`             | object       | Non         | Metadata pour les dashboards et logging via webhooks (Si configur√©s).                                        |
| `headers`              | object       | Non         | Headers compl√©mentaires pour l'authentification des tools calls.                                             |
| `identity`             | object       | Non         | Informations d'identit√© de l'utilisateur pour personnaliser le comportement de l'agent.                      |
| `asyncRagScore`        | boolean      | Non         | Si vrai, on n'attend pas le calcul du score RAG.                                                             |
| `attachementsFastMode` | boolean      | Non         | Si vrai, nous ignorons la vision sur les attachements                                                        |
| `hiddenMode`           | boolean      | Non         | Si vrai, la conversation ne sera pas visible dans l'interface utilisateur Devana.                            |

### V√©rifications et limites

Le syst√®me effectue automatiquement plusieurs v√©rifications :

- Maximum 4 messages par requ√™te
- Temp√©rature entre 0 et 1
- Pr√©sence obligatoire d'au moins un message utilisateur
- V√©rification des limites d'utilisation par utilisateur
- Validation des droits d'acc√®s √† l'agent

### Tool Calls

Les outils ajout√©s via l'interface par l'utilisateur sont appel√©s automatiquement.

#### Headers envoy√©s aux outils

```typescript
{
  "x-user-id": string; // ID de l'utilisateur actuel Devana (si disponible)
  "x-agent-id": string; // ID de l'agent actuel utilis√© pour la r√©ponse
  // Autres headers personnalis√©s ajout√©s lors de la requ√™te
}
```

#### Support de confirmation des tool calls

Le syst√®me prend en charge la confirmation d'ex√©cution d'outils via un format sp√©cial :

```
[tool:confirm:{"messageId":"ID_MESSAGE","confirm":true|false}]
```

### Objet Message

```typescript
{
  role: "user" | "system" | "assistant",
  content: string
}
```

### Exemple de Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent_id",
    "messages": [
      {"role": "user", "content": "Bonjour!"}
    ],
    "temperature": 0.7,
    "stream": false
  }'
```

## R√©ponses

### R√©ponse Standard

```typescript
{
  id: string;                  // ID du message
  object: "chat.completion";   // Type d'objet
  created: number;             // Timestamp
  model: string;               // ID du mod√®le utilis√©
  conversation_id: string;     // ID de conversation
  rag_score: number;           // Score de la r√©ponse (qualit√©)
  debug: {
    tool_calls: any[];         // Outils appel√©s durant la g√©n√©ration
    sources: any[];            // Sources utilis√©es pour la r√©ponse
    metadata: object;          // M√©tadonn√©es additionnelles
  };
  choices: [{
    index: number;
    message: {
      role: "assistant";
      content: string;
      tool_calls: null;
      score: {
        value: number;         // Score de la r√©ponse
      };
    };
    finish_reason: string | null;
    logprobs: null;
  }];
  usage: {
    prompt_tokens: number;     // Tokens dans le prompt
    completion_tokens: number; // Tokens dans la completion
    total_tokens: number;      // Total des tokens utilis√©s
  };
}
```

### R√©ponse en Streaming

Quand `stream: true`, l'API renvoie un flux d'√©v√©nements server-sent. Chaque √©v√©nement contient un delta de la r√©ponse :

```typescript
{
  id: string;
  object: "chat.completion.chunk";
  created: number;
  model: string;
  conversation_id: string;
  rag_score: number | null;    // Pr√©sent uniquement dans le dernier chunk
  debug: object | null;        // Pr√©sent uniquement dans le dernier chunk
  choices: [{
    index: number;
    delta: {
      content: string;
    };
    finish_reason: null | "stop";
    logprobs: null;
  }];
  usage: {                     // Mis √† jour dans chaque chunk
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

Le flux se termine par un chunk avec `finish_reason: "stop"` suivi d'un message `[DONE]`.

## Gestion des Erreurs

L'API utilise les codes de r√©ponse HTTP conventionnels :

- `400` - Mauvaise Requ√™te (param√®tres invalides, messages manquants, etc.)
- `403` - Interdit (acc√®s refus√© ou limite d'utilisation atteinte)
- `404` - Non Trouv√© (mod√®le ou conversation non trouv√©)
- `500` - Erreur Interne du Serveur

Le syst√®me impl√©mente une strat√©gie de retry avec backoff exponentiel pour les erreurs temporaires :

- Maximum 3 tentatives par d√©faut
- D√©lai initial de 500ms
- D√©lai doubl√© √† chaque nouvelle tentative

### Format de R√©ponse d'Erreur

```typescript
{
  error: string; // Message d'erreur d√©taill√©
}
```

## Limitation de D√©bit

L'API inclut une limitation de d√©bit bas√©e sur l'utilisateur :

- Les m√©triques utilisateur sont v√©rifi√©es avant le traitement des requ√™tes
- Quand les limites sont atteintes, retourne le code status 403

## Fonctionnalit√©s Sp√©ciales

### Contexte de Conversation

- Supporte la continuation des conversations existantes via `conversation_id`
- Maintient l'historique complet des conversations et les pi√®ces jointes
- Suit automatiquement l'utilisation des tokens (prompt et r√©ponse)
- Calcule et stocke les tokens de contexte pour optimiser les co√ªts

### Support des Fichiers

- Accepte les IDs de fichiers dans la requ√™te
- Maintient les associations de fichiers avec les messages de conversation
- Combine automatiquement les fichiers attach√©s √† la conversation et √† la requ√™te actuelle

### Configuration Agent/Mod√®le

- Supporte les agents AI personnalis√©s et la configuration Devana AI par d√©faut
- Param√®tres d'identit√© et de comportement configurables par agent
- Connectivit√© web optionnelle pour des r√©ponses am√©lior√©es
- Gestion des versions du chat par agent

### Surveillance et Analytique

- Suit les requ√™tes r√©seau d√©taill√©es
- Enregistre le statut de v√©rification et le scoring pr√©cis
- Supporte l'attribution des sources pour les r√©ponses
- Maintient des m√©triques d√©taill√©es d'utilisation des tokens
- Fournit un syst√®me de debug en temps r√©el

### Particularit√©s

- **Mode Chappie** disponible pour certains agents (v√©rification automatique)
- Gestion automatique de la langue selon les pr√©f√©rences utilisateur
- Support multi-mod√®les avec possibilit√© de sp√©cifier le mod√®le client
- V√©rification automatique des droits d'acc√®s aux agents
- Gestion des statuts de conversation (PENDING, DONE, ERROR)

### Param√®tre asyncRagScore

- Si `true`, la r√©ponse est renvoy√©e imm√©diatement sans attendre le calcul du score RAG. Le score RAG sera calcul√© en arri√®re-plan. Les r√©ponses n'incluront plus les sources, qui seront syst√©matiquement un tableau vide (`[]`).

# Exemples d'Utilisation

## Conversation Simple

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-agent",
    "messages": [
      {
        "role": "user",
        "content": "Quelle est la capitale de la France?"
      }
    ],
    "temperature": 0.7
  }'
```

### R√©ponse

```json
{
  "id": "msg_123abc",
  "object": "chat.completion",
  "created": 1698152365,
  "model": "default-agent",
  "conversation_id": "conv_456def",
  "rag_score": 0.95,
  "debug": {
    "tool_calls": null,
    "sources": null,
    "metadata": null
  },
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "La capitale de la France est Paris.",
        "tool_calls": null,
        "score": {
          "value": 0.95
        }
      },
      "finish_reason": "stop",
      "logprobs": null
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 8,
    "total_tokens": 17
  }
}
```

## Conversation avec Historique

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-agent",
    "conversation_id": "conv_456def",
    "messages": [
      {
        "role": "user",
        "content": "Et combien d'habitants y vivent?"
      }
    ]
  }'
```

## Utilisation du Streaming

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-agent",
    "messages": [
      {
        "role": "user",
        "content": "Raconte moi une histoire"
      }
    ],
    "stream": true
  }'
```

### R√©ponse (flux d'√©v√©nements)

```
data: {"id":"msg_789ghi","object":"chat.completion.chunk","created":1698152366,"model":"default-agent","conversation_id":"conv_456def","choices":[{"index":0,"delta":{"content":"Il"},"finish_reason":null,"logprobs":null}],"usage":{"prompt_tokens":5,"completion_tokens":1,"total_tokens":6}}

data: {"id":"msg_789ghi","object":"chat.completion.chunk","created":1698152366,"model":"default-agent","conversation_id":"conv_456def","choices":[{"index":0,"delta":{"content":" √©tait"},"finish_reason":null,"logprobs":null}],"usage":{"prompt_tokens":5,"completion_tokens":2,"total_tokens":7}}

data: {"id":"msg_789ghi","object":"chat.completion.chunk","created":1698152366,"model":"default-agent","conversation_id":"conv_456def","choices":[{"index":0,"delta":{"content":" une"},"finish_reason":null,"logprobs":null}],"usage":{"prompt_tokens":5,"completion_tokens":3,"total_tokens":8}}

...

data: {"id":"msg_789ghi","object":"chat.completion.chunk","created":1698152366,"model":"default-agent","conversation_id":"conv_456def","rag_score":0.87,"debug":{"tool_calls":[],"sources":null,"metadata":{}},"choices":[{"index":0,"delta":{"content":""},"finish_reason":"stop","logprobs":null}],"usage":{"prompt_tokens":5,"completion_tokens":150,"total_tokens":155}}

data: [DONE]
```

## Conversation avec Fichiers

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-agent",
    "messages": [
      {
        "role": "user",
        "content": "Analyse ce document"
      }
    ],
    "files": ["file_123abc"],
    "temperature": 0.3
  }'
```

## Utilisation d'un Agent Personnalis√© avec Debug et Identit√©

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent-specialise-juridique",
    "messages": [
      {
        "role": "user",
        "content": "Explique moi le concept de force majeure"
      }
    ],
    "lang": "fr",
    "custom": {
      "disableAutomaticIdentity": true
    },
    "identity": {
      "name": "Conseiller Juridique",
      "expertise": "Droit Civil"
    },
    "metadata": {
      "sessionContext": "consultation",
      "clientSector": "Assurance"
    }
  }'
```

## Confirmation d'Ex√©cution d'Outil

### Requ√™te

```bash
curl -X POST https://api.devana.ai/v1/chat/completions \
  -H "Authorization: Bearer VOTRE_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-agent",
    "conversation_id": "conv_456def",
    "messages": [
      {
        "role": "user",
        "content": "[tool:confirm:{\"messageId\":\"msg_123abc\",\"confirm\":true}]"
      }
    ]
  }'
```

## Gestion d'Erreur et Retry

### Exemple de R√©ponse d'Erreur

```json
{
  "error": "Limit reached"
}
```

# Conseils d'Utilisation Avanc√©s

1. **Gestion des Tokens**

   - Surveillez l'utilisation des tokens dans les r√©ponses pour estimer les co√ªts
   - Ajustez `max_tokens` selon vos besoins sp√©cifiques
   - Le syst√®me calcule automatiquement les tokens pour le prompt et la r√©ponse

2. **Optimisation des R√©ponses**

   - Utilisez une temp√©rature basse (0.1-0.3) pour des r√©ponses factuelles pr√©cises
   - Augmentez la temp√©rature (0.7-0.9) pour des r√©ponses plus cr√©atives et vari√©es
   - Ajustez `frequency_penalty` et `presence_penalty` pour contr√¥ler la r√©p√©tition

3. **Performance et Stabilit√©**

   - Utilisez le streaming pour une meilleure exp√©rience utilisateur sur les r√©ponses longues
   - Impl√©mentez une logique de retry c√¥t√© client (le serveur a d√©j√† un backoff exponentiel)
   - Pour les environnements critiques, surveillez activement les m√©triques d'utilisation

4. **Gestion du Contexte**

   - Stockez les `conversation_id` pour maintenir le contexte entre les sessions
   - Limitez le nombre de messages dans vos requ√™tes (maximum 4)
   - Utilisez les fichiers pour fournir plus de contexte sans augmenter le nombre de messages

5. **Debugging**

   - Activez les m√©tadonn√©es pour obtenir des informations d√©taill√©es sur le traitement
   - Utilisez le mode streaming pour voir la progression de la g√©n√©ration
   - Exploitez les informations de debug pour am√©liorer vos prompts

6. **S√©curit√©**
   - Utilisez des headers personnalis√©s pour l'authentification des tools calls
   - G√©rez correctement les confirmations d'ex√©cution pour les outils critiques
   - Validez les entr√©es utilisateur avant de les envoyer √† l'API
